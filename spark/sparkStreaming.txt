Overview
spark streaming is an extension of the core Spark API that enables scalable, high-throughput, fault-tolerant stream processing of live data streams.
Data can be ingested from many sources like kafka, flume, twitter, zerMQ, Kinesis, or TCP sockets, and can be processed using complex algorithms expressed
with high-level functions like map, reduce, join, and window. finally, processed data can be pushed out to filesystem, databases, and hive dashboards.
In fact, you can apply spark's machine learning and graph processing algorithms on data streams.


Internally, it works as follows, spark streaming receives live input data streams and divides the data into batches, which are then processed by 
the spark engine to generate the final stream of results in batches.


inputDataStream ===>> sparkStreaming ==> batches of input data ===>>> spark engine ===>>>> batches of processed data

spark streaming provides a high-level abstraction called discretized stream or DStream, which represents a continuous stream of data.
DStreams can be created either from input data streams from sources such as Kafka, Flume, and Kinesis, or by applying high-level operations on other DStreams.
Internally, a Dstream is represented as a sequence of RDDs.

this guide shows you how to start writing spark streaming programs with DStreams.
you can write spark streaming programs in scala, java, or python, all of which are presented in this guide.
you will find tabs throughout this guide that let you choose between code snippets of different languages.

Note: there are a few APIs that are either different or not avaiable in python.


A quick example
before we go into the details of how to write your own spark streaming program.
let's take a quick look at what a simple spark streaming program looks like,
let's say we want to count the number of words in text data received from a data server listening on a TCP socket.
all you need to do is as follows
    

First, we create a JavaStreamingContext object, which is the main entry point for all streaming functionality. 
we create a local streamingContext with two execution threads, and a batch interval of 1 second.

import org.apache.spark.*;
import org.apache.spark.api.java.function.*;
import org.apache.spark.streaming.*;
import org.apache.spark.streaming.api.java.*;
import scala.Tuple2;

//create a local StreamingContext with two working thread and batch interval of 1 second
SparkConf conf = new SparkConf().setMaster("local[2]").setAppName("NetworkWordCount")
JavaStreamingContext jssc = new JavaStreamingContext(conf,Duration.seconds(1))

using this context, we can create a DStream that represents streaming data from a TCP source, specified as hostname(e.g. localhost) and port (e.g. 9999)

//create a DStream that will connect to hostname:port, like localhost:9999
JavaReceiverInputDStream<String> lines = jssc.socketTextStream("localhost", 9999);

This lines DStream represents the stream of data that will be received from the data server. each record in this stream is a line of text.
Then we want to split the lines by space into words.


//split each line into words, DStream flatMap to another DStream.
JavaDStream<String> words = lines.flatMap(
    new FlatMapFunction<String, String>() {
        @Override public Iterable<String> call(String x) {
            return Arrays.asList(x.split(" "));
        }
    });



Discretized Streams (DStreams)
Discretized Stream or DStream is the basic abstraction provided by spark streaming.
it represents a continuous stream of data, either the input data stream received from source,
or the processed data stream generated by transforming the input stream.
internally, a DStream is represented by a continuous series of RDDs, which is spark's abstraction of an immutable, distribued database.
each RDD in a DStream contains data from a certain interval, as shown in the following figure.

DStream ===>>>  RDD@time1 ====>> RDD@time2 ====>>> RDD@time3 ===>>> RDD@time4 ===>>>

Any operations applied on a DStream translates to operations on the underlying RDDs.
For example, in the earlier example of converting a stream of lines to words,
the flatmap operation is applied on each RDD in the lines DStream to generate the RDDs of the words DStream.

this is shown as follows:

linesDstream  ===>> linesFrom0To1 ===> linesFrom1To2 ===> linesFrom2To3 ....

flatMap

wordSDstream ===> wordsFromTime0to1 ===> wordsFromTime1To2 ===> wordsFromTime2To3 ....

These underlying RDD transformations are computed by the spark engine.
The DStream operations hide most of these details and provide the developer with a higher-level API for convenience.
These operations are discussed in later sections.


Receiver Reliability
There can be two kinds of data sources based on their reliability, souces (like kafka and flume) allow the transferred data to be acknowledged.
If the system receiving data from these reliable sources acknowledges and received data correctly, it can be ensured that no data will be lost due to any kind of failure.
this leads to two kinds of receivers:
1. reliable receiver -- a reliable receiver correctly sends acknowledgement to a reliable source when the data has been received and stored in spark with replication.
2. unreliable receiver -- an unreliable receiver does not send acknowledgment to a source. this can be used for sources and do not support acknowledgment, or even for reliable sources
when one does not want or need to go into the complexity of acknowledgement.


Deploying applciations:
this section discusses the steps to deploy a spark streaming application.

Requirements
to run a spark streaming applications, you need to have the following.

Cluster with a cluster manager -- this is the general requirement of any spark application, adn discussed in the deployment mode.
    this is the general requirement of any spark application, and discussed in detail in the deployment guide.

Configuring sufficient memory for executors
    since the received data must be stored in memory, the executors must be configured with sufficient memroy to hold the received data. note that if you are doing 10 minute window operations, 
    the system has to keep at least last 10 minutes of data in memory. so the memory requirements for the application depends on the operations used in it.

Configuring write ahead logs 
    since spark 1.2, we have introduced write ahead logs for achieving strong fault-tolerant guarantees.if enabled, all the data received from a receiver gets written into a write ahead log in the configuration checkpoint directory. this prevents data loss on driver recovery, thus ensuing zero data loss (discussed in detail in the Fault-tolerance semantics section). this can be enabled by setting the configuration parameter spark.streaming.receiver.writeAheadLog.enable to true.
    However, these stronger semantics may come at the cost of the receiving throughput of individual receivers. this can be corrected by running more receivers in parallel to increase aggregate throughput.
    Additionally, it is recommended that the replication of the received data within spark be disabled when the write ahead log is enabled as the log is already stored in a replicated storage system.
    this can be done by setting the storage level for the input stream to StorageLevel.Memory_AND_DISK_SER>


Setting the max receiving rate 
    if the cluster resources is not large enough for the streaming application to process data as fast as it is being received. 
    the receivers can be rate limited by setting a maximum rate limit in terms of records/sec.
    see the configuration parameters spark.streaming.receiver.maxRate for receivers and spark.streaming.kafka.maxRatePerPartition for direct Kafka approach.
    in spark 1.5, we have introduced a feature called backpressure that eliminates the need to set this rate limit, as spark streaming automatically figures out the rate limits and dynamically adjusts them if the processing conditions change. this backpressure can be enabled by setting the configuration parameter spark.streaming.backpressure.enabled to true.



Monitoring Applications
Beyond spark's monitoring capabilities, there are additional capabilities specific to spark Streaming.
When a StreamingContext is used, the spark web UI shows an additional Streaming tab which shows statics about running receivers (whether receivers are active, number of records received, receiver error, etc) and completed batches (batch processing time, queueing delays, etc). this can be used to monitor the progress of the streaming application.

the following two metrics in web UI are particularly important:
processing time ----- the time to process each batch of data.
scheduling delay ----- the time a batch waits in a queue for the processing of previous batches to finish.

if the batch processing time is consistently more than the batch interval and/or the queueing delay keeps increasing, then it indicates that the system is not able to process the batches as fast they are being generated and is falling behind. in that case, consider reducing the batch processing time.


Checkpointing
a streaming application must operate 24/7 and hence must be resillient to failures unrelated to the application logic (e.g. system failures, JVM crashes, etc).
For this to be possible, spark Streaming needs to be checkpoint enough information to a fault-tolerant storage system such that it can recover from failures.
there are two types of data that are checkpointed.

Metadata checkpointing -- saving of the information defining the streaming computation to fault-tolerant storage like HDFS.
this is used to recover from failure of the node running the driver of the streaming application(discussed in detail later).
Metadata includes:
    configuraiton
    DStream operation
    incomplete batches

Data checkpointing
saving of the generated RDDs to reliable storage.
this is necessary in some stateful transformations that combine data across multiple batches.
in such transformations, the generated RDDs depend on RDDs of previous batches, which causes the length of the dependency chain to keep increasing with time.


















